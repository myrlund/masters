\chapter{Evaluation}

\label{Chapter4}

\lhead{Chapter 4. \emph{Evaluation}}

\section{Description of the evaluated dataset} % (fold)
\label{sec:description_of_the_evaluated_dataset}

% section description_of_the_evaluated_dataset (end)

\section{Prerequisites} % (fold)
\label{sec:prerequisites}

For any of this user adaptation to have any actual use, we will need to find out whether there is actually significant variation in feature adoption between clusters; it matters little whether users exhibit differing behavior with regard to existing functionality, if this behavior yields no basis for predicting feature adoption in the future. We need an inductive bias.

An easy way of discovering whether this inductive bias is present in the user base is to simply log whether there is significant variance across the user groups in their users' adoption of new features. Thus, this question of whether the predictive bias actually exists will be included as a central part of the actual experiment itself, and be thoroughly discussed through the next sections.

% section prerequisites (end)

\section{Experimental setup} % (fold)
\label{sec:experimental_setup}

\emph{Condensed version.}

The main experiment determines whether an alteration of the service affects different kinds of users differently.
More specifically, do users in clusters $C_1, \ldots, C_n$ employ function $f$ in significantly differing ways? We find out without touching a large percentage of the users, and can use the results to individually enable features for users that

We break the process into three steps, each elaborated on in~\ref{sub:clustering_of_users}, \ref{sub:a_b_testing_features}, and \ref{sub:evaluating_the_results}:

\begin{enumerate}
  \item A/B test feature $f$ on a percentage of all users.
  \item Separate users into clusters $C_1, \ldots, C_n$.
  \item Analyze results: did the test results vary significantly between clusters?
\end{enumerate}

If the final answer is ``yes'', we can store our results and use them as a basis for adaptating the interface for each user.

\subsection{A/B testing features} % (fold)
\label{sub:a_b_testing_features}

\emph{@TODO: Brief description of how to fit the actual testing regime into the production system.}

% subsection a_b_testing_features (end)

\subsection{User clustering} % (fold)
\label{sub:clustering_of_users}

\emph{@TODO: Describe methods and algorithms used. What kind of results are expected?}

% subsection clustering_of_users (end)


% section experimental_setup (end)

\section{Adapting the user interface} % (fold)
\label{sec:adapting_the_user_interface}

The adaptation component consists of two important steps.

\begin{enumerate}
  \item Determine which type of user (ie. \emph{cluster}) will have the most to gain from having feature $f$ enabled.
  \item Tagging the users with cluster and enabling features for
\end{enumerate}

\subsection{Evaluating the results} % (fold)
\label{sub:evaluating_the_results}

Seeing as the application does not yield any direct and real payoff, we will evaluate the performance of each variation relatively, using some general key metrics. This approach is described in~\cite{Yue2012}.

For appear.in, the most important key metrics are ``time on site'' and ``number of visits in the last $n$ days''. These are combined in providing a relative success metric for each variation.

\emph{@TODO: Verify key metrics.}

% subsection evaluating_the_results (end)


% section adapting_the_interface (end)

\section{Results} % (fold)
\label{sec:results}

% section results (end)

\section{Discussion} % (fold)
\label{sec:discussion}

% section discussion (end)
